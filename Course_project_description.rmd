---
title: "Course Project "
date: "3-20-23"
author: "Shawheen Naderi"
output: html_document
---

```{r, echo=FALSE, include=FALSE}
library("factoextra")
library(purrr)
library("lmerTest")
library(aod)
library(ggplot2)
```

# Abstract

This project aims to test whether there are interaction between stimuli presented on screens located on the right and left side of a mouse as it performs a task. Neuronal activity in the visual cortex are used to gauge these interactions. Three clusters of data are employed as it is theorized that there are three types of neurons in this region of the brain. Then an ANOVA test is conducted to highlight possible differences. Finally, the data are used in a prediction task which is undertaked using logistic regression. 

# Introduction

The brain can easily be argued to be one of the most interesting phenomena there exists in nature and the powerful tools of statistics can be utilized to help explore this organ in both animal and human specimens. Given the immense complexity of the aforementioned object of study, a researcher must concentrate on a narrow substructure of the brain and then conduct a series of experiments and analyses to push the boundaries of what is known about the brain. In this particular report, it is of interest to focus on the visual cortex of a mouse brain and study the effects that presenting visual stimuli on a screen to its left and right side and a then requiring a task has on recorded neural activity in the studied region.

The paramount question of this analyses is to determine how neurons in the visual cortex behave and respond to stimuli that are presented by screens on the left and right sides. Additionally, it is of interest to determine if the two possible sides of visual stimuli interact with each other and analysis of variance can provide a systematic method of determining the interactions.

Finally, a task of developing a prediction model that incorporates neural activity and stimuli will be implemented with the aim of predicting the outcome of a trial.

# Background

The experiment can be broken down into 39 different sessions composed of hundreds of trials. Each session is the product of one of ten particular mice on a specific date. For example, session 1, 2, and 3 may all be the same mouse on different dates. The individual trials are events in which the mice are presented a stimuli on screens positioned to the left and right side where the stimuli can vary in four discrete intensities: [0, .25, .5, 1] where 0 is no stimuli and 1 is the most extreme level of stimuli. Furthermore, the mice are presented a combination of stimuli on the left and right screens. For instance, on the left screen they may be exposed to .25 and on the right screen they may be exposed to .5. Hence, there are $4 * 4 = 16$ possible combinations of stimuli that can be presented to the mouse.

The mouse was then required to perform a task of turning a wheel where it received a water reward for selecting the screen with the highest contrast (level of stimuli), a penalty for incorrect selections, and a reward for keeping the wheel steady if a stimuli (both right and left screens were set to value 0) was not present [1].

The mice's neural activities were recorded using Neuropixels probes which are instruments that stick into the brain tissue in a similar manner that one may stick a nail into an orange from the exterior of the orange [1]. The fact that the probes are inserted in this manner limit the precision of which neurons are targeted and recorded. Nevertheless, certain regions of the brain may be localized and particular neurons may be focused in on for recording the number of times these particular neurons fire in a certain time interval. It should be noted that across sessions the particular neurons being measured vary as new probes must be inserted during each new session.

# Descriptive analysis.

The data set in this analysis will focus on five different sessions: "Cori" "2016-12-14","Cori" "2016-12-17", "Cori" "2016-12-18", "Forssmann" "2017-11-01", and "Forssmann" "2017-11-02" where the name name of the mouse studied and the date of the experiment is displayed.

The data also has information for each of the possible trials. These include: `feedback_type:` type of the feedback, 1 for success and -1 for failure, `contrast_left:` contrast of the left stimulus, `contrast_right:` contrast of the right stimulus, `time:` centers of the time bins for spks, and `spks:` numbers of spikes of neurons in the visual cortex in time bins defined in time. To further clarify, a spike is considered to be an [action potential](https://en.wikipedia.org/wiki/Action_potential) of a neuron or in more common terms it can be thought of as the neuron firing [2].

Since the focus of our study is to determine interactions we will be interested in the mean firing rate as a response variable. The mean firing rate of the $lth$ trial of the $kth$ session is $\text{mean firing rate} = \frac{\text{total number of spikes}}{\text{number of neurons in session} \times \text{time}}$. Where time is binned at .4 seconds.

The structure of the data needed to calculate the mean firing rates for an individual session can be thought of as a rank 3 tensor where the dimensions correspond to the time, trial, and specific neuron. To make this more tangible, session one can be thought of as 214 matrices since there are 214 trials. Each of these matrices has the rows as the number neurons which will be 178 rows since there are 178 neurons and 39 columns for the different time bins. The entries of the matrices are the number of spikes a neuron has during a particular time frame. An illustration is below.

![](images/Screen%20Shot%202023-03-19%20at%206.41.27%20PM.png)

We then took this rank 3 tensor and compressed it to a rank 2 tensor by averaging across the trial data which provided insight into relevant patterns such as brain activity of mice of different sessions varies.


```{r, echo=FALSE}
session=list()
for(i in 1:5){
  session[[i]]=readRDS(paste('~/STA-207/session',i,'.rds',sep=''))
  
}
```

```{r, echo=FALSE}
# define a function that takes session ID as an argument
process_session <- function(session_id) {
  
  # make a rank 3 tensor from neuron and time matrix
  n.trials <- length(session[[session_id]]$spks)
  rank3_tensor <- lapply(1:n.trials, function(i) session[[session_id]]$spks[[i]])
  
  # make a rank 2 tensor by averaging over the time dimension
  rank2_tensor_sum <- Reduce("+", rank3_tensor)
  rank2_tensor_avg <- rank2_tensor_sum / length(rank3_tensor)
  #print(length(rank3_tensor))
  
  # return the tensors and the barplot object
  return(list(rank3_tensor = rank3_tensor, rank2_tensor_sum = rank2_tensor_sum, rank2_tensor_avg = rank2_tensor_avg))
}
```

```{r, echo=FALSE}
# run the function for each session and assign variable names based on session ID
for (session_id in 1:5) {
  tensors <- process_session(session_id)
  assign(paste0("rank3_tensor_", session_id), tensors$rank3_tensor)
  assign(paste0("rank2_tensor_sum_", session_id), tensors$rank2_tensor_sum)
  assign(paste0("rank2_tensor_avg_", session_id), tensors$rank2_tensor_avg)
}
```

```{r, echo=FALSE, results='asis'}
par(mfrow=c(2,3))
barplot(colSums(rank2_tensor_avg_1), main = "session 1 (Cori)", ylab = "count", xlab = "time")
barplot(colSums(rank2_tensor_avg_2), main = "session 2 (Cori)", ylab = "count", xlab = "time")
barplot(colSums(rank2_tensor_avg_3), main = "session 3 (Cori)", ylab = "count", xlab = "time")
barplot(colSums(rank2_tensor_avg_4), main = "session 4 (Forssmann)", ylab = "count", xlab = "time")
barplot(colSums(rank2_tensor_avg_5), main = "session 5 (Forssmann)", ylab = "count", xlab = "time")
```

We notice that the peaks at the various plots range quite a bit. This could indicate that the mouse is perhaps more engaged at different days since engagement has an impact on performance [1]. When a mouse is less engaged the stimuli have less of an effect on neural ciruits [1]. Or, since there appears to be a downward trend, the mice could be learning and hence when they complete the task their neurons don't need to fire as much; maybe they become more efficient because of [Hebbian theory](https://en.wikipedia.org/wiki/Hebbian_theory).

We further theorize that there are different types of neurons in the data and will implement [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering) to cluster the neurons into different groups based on their mean firing rate [3]. Our choice of groups will be 3 given that we know there are three types of neurons: motor, sensory, and interneurons [4]. However, our beliefs could be wrong and the different clusters could be the result of other phenomena at play. Perhaps the spatial location of the nuerons in the brain impact the firing rate. Additionally, we scaled the data by subtracting off the means of an individual neuron across all trials in a session since we noticed that on some sessions the mice have more active spikes. Then applied the clustering algorithm and observed clusters of 66, 1077, and 15 neurons. We also notice that the smallest cluster only has neurons that are in sessions two and three. 

After this we split the data into three different clusters and performed and three different analysis of variance tests for each type of neuron. 

```{r, echo=FALSE}
#add a unique row number to the data frames and the session id
rank2_tensor_avg_1 <- data.frame(rank2_tensor_avg_1)
rank2_tensor_avg_1$row_id <- seq.int(nrow(rank2_tensor_avg_1))
rank2_tensor_avg_1$session_id <- rep(1, dim(session[[1]]$spks[[1]])[1])

rank2_tensor_avg_2 <- data.frame(rank2_tensor_avg_2)
rank2_tensor_avg_2$row_id <- seq.int(nrow(rank2_tensor_avg_2))
rank2_tensor_avg_2$session_id <- rep(2, dim(session[[2]]$spks[[1]])[1])

rank2_tensor_avg_3 <- data.frame(rank2_tensor_avg_3)
rank2_tensor_avg_3$row_id <- seq.int(nrow(rank2_tensor_avg_3))
rank2_tensor_avg_3$session_id <- rep(3, dim(session[[3]]$spks[[1]])[1])

rank2_tensor_avg_4 <- data.frame(rank2_tensor_avg_4)
rank2_tensor_avg_4$row_id <- seq.int(nrow(rank2_tensor_avg_4))
rank2_tensor_avg_4$session_id <- rep(4, dim(session[[4]]$spks[[1]])[1])

rank2_tensor_avg_5 <- data.frame(rank2_tensor_avg_5)
rank2_tensor_avg_5$row_id <- seq.int(nrow(rank2_tensor_avg_5))
rank2_tensor_avg_5$session_id <- rep(5, dim(session[[5]]$spks[[1]])[1])
```

```{r, echo=FALSE}
#stich all the average sessions together so we can perform k means clustering 
binded_df <- rbind(rank2_tensor_avg_1, rank2_tensor_avg_2, rank2_tensor_avg_3, rank2_tensor_avg_4, rank2_tensor_avg_5)
```

```{r, echo=FALSE}
#standardize by subtracting the mean but we should transpose it first and then untranpose to use the scale function
#use 1 to 39 columns to prevent the labels from getting involved
hi <- binded_df[,1:39]
#subtract by average to remove the shape because perhaps a mouse is feelign different one of the days 
scaled_matrix <- t(scale(t(hi), center = TRUE, scale = FALSE))
```


```{r, echo=FALSE}
final <- kmeans(scaled_matrix, 3, nstart = 25)
```

```{r, echo=FALSE}
#label the rows of neurons by their classification 
clustered_df <- cbind(binded_df, final$cluster)
```

```{r, echo=FALSE}
#split the data frame into 3 based on the cluster type


# Split the data frame based on the cluster column
df_list <- split(clustered_df, f = clustered_df$`final$cluster`)

cluster1 <- df_list[["1"]]
cluster2 <- df_list[["2"]]
cluster3 <- df_list[["3"]]

#get desired indices where j is session and i cluster
for (i in 1:3) {
  for (j in 1:5) {
    cluster_name <- paste("cluster", i, sep ='')
    kappa_name <- paste("cluster_", i, "_session_", j, sep ='')
    alpha_name <- paste("n.indices_", kappa_name, sep = '')

    # Get the data frame by name
    cluster <- get(cluster_name)
    
    # Subset the data frame by session ID
    kappa <- cluster[cluster$session_id == j, ]
    
    # Get the row IDs
    n.indices <- kappa$row_id
    
    # Store the row IDs in a variable with the desired name
    assign(alpha_name, n.indices)
  }
}
```


```{r, echo=FALSE}
for (i in 1:3) {
  # Create an empty data frame to store the results
  results_df <- data.frame(feedback_type = numeric(),
                            contrast_left = numeric(),
                            contrast_right = numeric(),
                            ID = numeric(),
                            firingrate = numeric())
  
  for (j in 1:5) {
    kappa_name <- paste("n.indices_cluster_", i, "_session_", j, sep ='')
    firingrate_name <- paste("firingrate_", kappa_name, sep = '')
    kappa <- get(kappa_name)
    
    #make sure it is not an empty list
    if (length(kappa) != 0) {
    
      ID=j
      t=0.4 # from Background 
      
  
      n.trials=length(session[[ID]]$spks)
      n.neurons=dim(session[[ID]]$spks[[kappa[1]]])[1]
      
      # Obtain the firing rate
      firingrate=numeric(n.trials)
      for(k in 1:n.trials){  #
        firingrate[k]=sum(session[[ID]]$spks[[k]][kappa,])/n.neurons/t #get the neurons at index kappa
        
        # Append the values to the data frame
        new_row <- data.frame(feedback_type = session[[ID]]$feedback_type[k],
                              contrast_left = session[[ID]]$contrast_left[k],
                              contrast_right = session[[ID]]$contrast_right[k],
                              ID = ID,
                              firingrate = firingrate[k])
        results_df <- rbind(results_df, new_row)
      }
    }
  }
  
  # Assign the data frame to a variable with a name that includes the value of i
  assign(paste("results_df_", i, sep = ""), results_df)
}

```


# Inferential Analysis

We utilized a split-plot design for our model because we have interaction terms and we want to include the session as a random effect which is seen in the below equation since we wish to be able to apply our model across other sessions and we have already observed that different session have different spike activity. We should note this model is unbalanced. 


$$Y_{kl} = \mu_{\cdot\cdot} + \alpha_{i[kl]} +\beta_{j[kl]} + (\alpha\beta)_{ij[kl]}+ \eta_{k} + \epsilon_{kl}, \ k=1,\ldots, 5, \ i=1,\ldots, 4, \ j=1,\ldots, 4, \ l=1,\ldots, total \ trials \ of \ kth \ session$$ where (i) $\sum \alpha_i =0$, (ii) $\beta_j$ are i.i.d. $N(0,\sigma_{\beta}^2)$, (iii) $\sum_i (\alpha\beta)_{ij} =0$ for any $j$, (iv) \ $ (\alpha\beta)*{ij}* \sim N(0,(1-1/a)\sigma\^2{\alpha\beta}) \ $, (v) ${\rm cov}( (\alpha\beta)_{ij}, (\alpha\beta)_{i'j})= -\sigma^2_{\alpha\beta}/a$, (vi) ${\rm cov}( (\alpha\beta)_{ij}, (\alpha\beta)_{i'j'})=0$, if $i\neq i'$ and $j\neq j'$, (vii)$\{\epsilon_{ijk}\}$ are i.i.d. $N(0,\sigma^2)$, and (viii) $\{ \beta_j\}$, $\{(\alpha\beta)_{ij}\}$, $\{\epsilon_{ijk} \}$ are mutually independent.

In this model, $\alpha_{i[kl]}$ represents the left contrast (which can take on values [0,.25.5,1]) for the $lth$ trial of the $kth$ session. $\beta_{j[kl]}$ represents the right contrast (which can take on values [0,.25.5,1]) for the $lth$ trial of the $kth$ session. The (\alpha\beta)*{ij[kl]} represents the interaction term. The* \eta{k} represents the random effect intercept of the $kth$ trial. The outcome $Y_{kl}$ is the mean firing rate of the $k$th subject under $i$th condition and $j$th factor. The mean effect $\mu_{\cdot\cdot}$ represents the average of the mean firing rate across all sessions and trials. The errors $\epsilon_{l, k}$ capture any unexplained effects on mean firing rate.

This is what the data set for cluster 1 looks like which we will input into out model. 
```{r, echo =FALSE}
head(results_df_1)
```



Suppose that we are interested in testing the presence of interactions. $$
H_0: (\alpha\beta)_{ij[kl]}=0 \ {\rm v.s.} \ H_1: {\rm not \ all \ } (\alpha\beta)_{ij[kl]} \ {\rm are \ zero}.
$$ We now need to use the following framework for testing. \* Full model: $$Y_{kl} = \mu_{\cdot\cdot} + \alpha_{i[kl]} +\beta_{j[kl]} + (\alpha\beta)_{ij[kl]}+ \eta_{k} + \epsilon_{kl}, \ k=1,\ldots, 5, \ i=1,\ldots, 4, \ j=1,\ldots, 4, \ l=1,\ldots, total \ trials \ of \ kth \ session$$ \* Reduced model: $$Y_{kl} = \mu_{\cdot\cdot} + \alpha_{i[kl]} +\beta_{j[kl]} + \eta_{k} + \epsilon_{kl}, \ k=1,\ldots, 5, \ i=1,\ldots, 4, \ j=1,\ldots, 4, \ l=1,\ldots, total \ trials \ of \ kth \ session$$

We will use the [likelihood ratio test](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/) to help determine if there is an interaction which will do this by deciding the parameters of the model that make it most probable to correctly match the provided data. A strenth of the likelihood ratio test is that it relies on large sample size and the sample size is rather large. We will first create a full model and then drop the interaction term to get a reduced model. From there, we can determine if the probability of matching the data is statistically significant by including the interaction term. If if is then we will decide to keep the interaction term and conclude that there is a statistical significant difference. If it is not then we will conclude that it is not statistically significant and that there is not an interaction effect.

Likelihood ratio test. ${\rm LR}=-2 \big[ \log \mathcal{L}({\rm reduced}) -\log \mathcal{L}({\rm full}) \big]$. We have that ${\rm LR} \sim \chi^2_{K-k}$, under $H_0$ and large $n$.

Once we performed the Likelihood ratio test we calculated test and at significane level $\alpha = .05$ we determined that the cluster 1 and cluster 3 had interactions between the left and right sides since their p-values from the chi-squared tests were respectively: 0.0004615 and 0.028784925730141. Conversely, cluster 2 had a p-value of 0.8239 so we conclude that there is not an interaction effect between the left and right side in this cluster of neurons. 

There could be many reasons for this and someone with better domain knowledge may be able to offer more insight. 

Additionally, we provided a set graphs illustrating the interaction terms to help those visualize the underlying phenomena. 

```{r, echo=FALSE}
#cluster 1 
# Fit full and reduced models
full_model_1 <- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + as.factor(contrast_left) * as.factor(contrast_right) + (1|ID), data = results_df_1)
reduced_model_1 <- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + (1|ID), data = results_df_1)

# Perform ANOVA to test for interaction
#aov1 = anova(full_model_1, reduced_model_1)

#calculate the log likelihood 
#lrt1 = -2*(aov1$logLik[2] - aov1$logLik[1])
#aov1
```

```{r, echo=FALSE}
#cluster 2 
# Fit full and reduced models



full_model_2<- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + as.factor(contrast_left) * as.factor(contrast_right) + (1|ID), data = results_df_2)
reduced_model_2 <- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + (1|ID), data = results_df_2)

# Perform ANOVA to test for interaction
#aov2 = anova(full_model_2, reduced_model_2)

#calculate the log likelihood 
#aov2

```


```{r, echo=FALSE}
#cluster 3 
# Fit full and reduced models
full_model_3<- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + as.factor(contrast_left) * as.factor(contrast_right) + (1|ID), data = results_df_3)
reduced_model_3 <- lmer(firingrate ~ as.factor(contrast_left) + as.factor(contrast_right) + (1|ID), data = results_df_3)

# Perform ANOVA to test for interaction
#aov3 = anova(full_model_3, reduced_model_3)

#calculate the log likelihood 
#lrt3 = -2*(aov3$logLik[2] - aov3$logLik[1])
#print(paste("lrt", lrt3, "chi square", aov3$`Pr(>Chisq)`))

```


```{r, echo=FALSE}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

with(results_df_1, interaction.plot(x.factor = contrast_left, 
                                     trace.factor = contrast_right, 
                                     response = firingrate,
                                     main = "Cluster 1",
                                     ylab = "Firing Rate",
                                     col = c("green", "red", "blue", "grey")
                                    )
     )


with(results_df_2, interaction.plot(x.factor = contrast_left, 
                                     trace.factor = contrast_right, 
                                     response = firingrate,
                                     main = "Cluster 2",
                                     ylab = "Firing Rate",
                                     col = c("green", "red", "blue", "grey")
                                    )
     )


with(results_df_3, interaction.plot(x.factor = contrast_left, 
                                     trace.factor = contrast_right, 
                                     response = firingrate,
                                     main = "Cluster 3",
                                     ylab = "Firing Rate",
                                     col = c("green", "red", "blue", "grey")
                                    )
     )


```


# sensitivity analysis

```{r, echo=FALSE}
# Obtain the residuals from the ANOVA fit
bluh1 = summary(full_model_1)
residuals1 = bluh1$residuals
qqnorm(residuals1, main = "QQplot for model 1");qqline(residuals1)
```

```{r, echo=FALSE}
bluh2 = summary(full_model_2)
residuals2 = bluh2$residuals
qqnorm(residuals2, main = "QQplot for model 2");qqline(residuals2)
```

```{r, echo=FALSE}
bluh3 = summary(full_model_3)
residuals3 = bluh3$residuals
qqnorm(residuals3, main = "QQplot for model 3");qqline(residuals3)

```

An issue we notice is that the residuals are not normally distrbuted based on the QQ plot which is one of the possible departures from out model assumptions. This may not be ultimately damning because even if we deviate from out model assumtpions our model may be adequate to allow for a decent approximation. In short, all models are wrong but some are useful. 

To address some possible departures from model assumptions it is very important to collaborate with neuroscientists who are experts in this field since we would not want to employ some sort of mathematical transformation that would be illogical to do on the given data. For instance, one may desire to implement a Box-Cox transformation but that would be unwise as it is primarly mathematical and may not make sense to actually use on this data. 

Additionally, we also previously noted that we are using the random effects for the sessions to allow us to generalize the model across other sessions. 



# Predictive modeling 

We will implement a prediction model using Logistic regression since it is fairly easy to use and the computational requirments are low compared to more sophisticated models such as deep neural networks which require graphical processing units to run and need more complex libraries such as TensorFlow or PyTorch.

Since prediction primarly relies on model performance, unlike the above inference where assumptions and deeper insight into what the model is doing are key, we will stick to just evaluating how well it performs on the data. 

We will evaluate the model on the first 100 sessions of data and so when we train it we will avoid using these sessions. 

```{r, echo=FALSE}
#get the training data 
for (i in 1:3) {
  # Create an empty data frame to store the results
  results_df <- data.frame(feedback_type = numeric(),
                            contrast_left = numeric(),
                            contrast_right = numeric(),
                            ID = numeric(),
                            firingrate = numeric())
  
  for (j in 1:5) {
    kappa_name <- paste("n.indices_cluster_", i, "_session_", j, sep ='')
    firingrate_name <- paste("firingrate_", kappa_name, sep = '')
    kappa <- get(kappa_name)
    
    #make sure it is not an empty list
    if (length(kappa) != 0) {
    
      ID=j
      t=0.4 # from Background 
      
  
      n.trials=length(session[[ID]]$spks)
      n.neurons=dim(session[[ID]]$spks[[kappa[1]]])[1]
      
      # Obtain the firing rate
      firingrate=numeric(n.trials)
      for(k in 100:n.trials){  #
        firingrate[k]=sum(session[[ID]]$spks[[k]][kappa,])/n.neurons/t #get the neurons at index kappa
        
        # Append the values to the data frame
        new_row <- data.frame(feedback_type = session[[ID]]$feedback_type[k],
                              contrast_left = session[[ID]]$contrast_left[k],
                              contrast_right = session[[ID]]$contrast_right[k],
                              ID = ID,
                              firingrate = firingrate[k])
        results_df <- rbind(results_df, new_row)
      }
    }
  }
  
  # Assign the data frame to a variable with a name that includes the value of i
  assign(paste("training_df_", i, sep = ""), results_df)
}

```

```{r, echo=FALSE}
#get the testing data 
for (i in 1:3) {
  # Create an empty data frame to store the results
  results_df <- data.frame(feedback_type = numeric(),
                            contrast_left = numeric(),
                            contrast_right = numeric(),
                            ID = numeric(),
                            firingrate = numeric())
  
  for (j in 1:5) {
    kappa_name <- paste("n.indices_cluster_", i, "_session_", j, sep ='')
    firingrate_name <- paste("firingrate_", kappa_name, sep = '')
    kappa <- get(kappa_name)
    
    #make sure it is not an empty list
    if (length(kappa) != 0) {
    
      ID=j
      t=0.4 # from Background 
      
  
      n.trials=length(session[[ID]]$spks)
      n.neurons=dim(session[[ID]]$spks[[kappa[1]]])[1]
      
      # Obtain the firing rate
      firingrate=numeric(n.trials)
      for(k in 1:100){  #
        firingrate[k]=sum(session[[ID]]$spks[[k]][kappa,])/n.neurons/t #get the neurons at index kappa
        
        # Append the values to the data frame
        new_row <- data.frame(feedback_type = session[[ID]]$feedback_type[k],
                              contrast_left = session[[ID]]$contrast_left[k],
                              contrast_right = session[[ID]]$contrast_right[k],
                              ID = ID,
                              firingrate = firingrate[k])
        results_df <- rbind(results_df, new_row)
      }
    }
  }
  
  # Assign the data frame to a variable with a name that includes the value of i
  assign(paste("testing_df_", i, sep = ""), results_df)
}

```

```{r, echo=FALSE}
#change the -1 to 0 in the training and testing data
training_df_1[1][training_df_1[1] == -1] = 0 
training_df_2[1][training_df_2[1] == -1] = 0 
training_df_3[1][training_df_3[1] == -1] = 0 

testing_df_1[1][testing_df_1[1] == -1] = 0 
testing_df_2[1][testing_df_2[1] == -1] = 0 
testing_df_3[1][testing_df_3[1] == -1] = 0 
```





```{r, echo=FALSE}
#train the model 
mylogit1 <- glm(feedback_type ~ contrast_left + contrast_right + ID, data = training_df_1, family = "binomial" )
```



```{r, echo=FALSE}
predict_list_1 <- predict(mylogit1, newdata = testing_df_1, type = "response")
```


```{r, echo=FALSE}
#see the differences between the two and round 
same1 = sum(testing_df_1$feedback_type == round(predict_list_1))


accuracy_1 = same1 / length(predict_list_1)
```


```{r, echo=FALSE}
#train the model 
mylogit2 <- glm(feedback_type ~ contrast_left + contrast_right + ID, data = training_df_2, family = "binomial" )
```

```{r, echo=FALSE}
predict_list_2 <- predict(mylogit2, newdata = testing_df_2, type = "response")
```


```{r, echo=FALSE}
#see the differences between the two and round 
same2 = sum(testing_df_2$feedback_type == round(predict_list_2))


accuracy_2 = same2 / length(predict_list_2)

```


```{r, echo=FALSE}
#train the model 
mylogit3 <- glm(feedback_type ~ contrast_left + contrast_right + ID, data = training_df_3, family = "binomial" )
```

```{r, echo=FALSE}
predict_list_3 <- predict(mylogit1, newdata = testing_df_3, type = "response")
```


```{r, echo=FALSE}
#see the differences between the two and round 
same3 = sum(testing_df_3$feedback_type == round(predict_list_3))


accuracy_3 = same3 / length(predict_list_3)

```


```{r, echo=FALSE}
#make a model where we do not incorporate the different trials and compare the performance 
training_df = rbind(training_df_1, training_df_2, training_df_3)
testing_df = rbind(testing_df_1, testing_df_2, testing_df_3)

#train the model 
mylogit <- glm(feedback_type ~ contrast_left + contrast_right + ID, data = training_df, family = "binomial" )

predict_list <- predict(mylogit1, newdata = testing_df, type = "response")

#see the differences between the two and round 
same = sum(testing_df$feedback_type == round(predict_list))


total_accuracy = same / length(predict_list)


#it is more accurate, perhaps there are more training data 
```

```{r, echo=FALSE}
barplot(c(accuracy_1, accuracy_2, accuracy_3, total_accuracy), names.arg = c("model 1", "model 2", "model 3", "total"),
        xlab = "models", ylab = "accuracies", main = "accuracies across models")
```

We developed 3 different models correpsonding to the three respective clusters and a fourth model for all of the neurons and notice that all their accuracies' are around 60%. It should be noted that the reason some models may be more accurate is that they have more training data as there are more neurons. Nevertheless, they all performed better than chance. 

# Conclusion

Ultimately, this project had a limited scope given the time constraints and lack both of domain and statistical knowledge. Nevertheless, it seems that this project provides insight into the workings of the visual cortex of the brain and can help provide motivation for further analysis. Some key findings were that there appears to be interaction effects with the right and left side of screen presented stimuli across certain clusters of neurons. 

# Reference {.unnumbered}

I just put the links for the references since it would be an inefficient use of time to format them in a pretty manner 

[1] <https://www.nature.com/articles/s41586-019-1787-x>

[2] <https://en.wikipedia.org/wiki/Action_potential>

[3] https://en.wikipedia.org/wiki/K-means_clustering

[4] https://www.healthline.com/health/neurons#:~:text=In%20terms%20of%20function%2C%20scientists,sensory%2C%20motor%2C%20and%20interneurons.

[5] https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/
